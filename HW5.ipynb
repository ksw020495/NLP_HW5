{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45cfc5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12741023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninput_file = open('translation2019zh_valid.json', encoding = 'utf8')\\nfor line in input_file.readlines():\\n    num = num + 1\\n    #data.append(json.loads(line))\\ninput_file.close()\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#讀檔\n",
    "\n",
    "num = 0\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_size = 50\n",
    "test_size = 10\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "input_file = open('translation2019zh_train.json', encoding = 'utf8')\n",
    "for line in input_file.readlines():\n",
    "    num = num + 1\n",
    "    if num > train_size + test_size:\n",
    "        break\n",
    "    elif num > train_size:\n",
    "        test_data.append(json.loads(line))\n",
    "    else:\n",
    "        train_data.append(json.loads(line))\n",
    "input_file.close()\n",
    "\n",
    "'''\n",
    "input_file = open('translation2019zh_valid.json', encoding = 'utf8')\n",
    "for line in input_file.readlines():\n",
    "    num = num + 1\n",
    "    #data.append(json.loads(line))\n",
    "input_file.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1738b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cc7152ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#簡轉繁\\nfrom opencc import OpenCC\\n\\ncc = OpenCC('s2t')\\n\\nfor data in train_data:\\n    data['chinese'] = cc.convert(data['chinese'])\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#簡轉繁\n",
    "from opencc import OpenCC\n",
    "\n",
    "cc = OpenCC('s2t')\n",
    "\n",
    "for data in train_data:\n",
    "    data['chinese'] = cc.convert(data['chinese'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d0c01d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料檔，並將所有單字整理為字典，分別為英文及中文字典，注意，英文為字母的集合，非單字(Word)\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for item in train_data:\n",
    "    input_text = item['english']\n",
    "    target_text = '\\t' + item['chinese'] + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fe0b0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。\\n', '\\t他还把宣扬自己思想的所谓《绿皮书》称作“新福音书”。\\n', '\\t微风推着我去爱抚它的长耳朵\\n', '\\t它们的先烈们的鲜血是白流了…\\n', '\\t最后，在1月31日，湖人将前往汽车城底特律挑战活塞队，活塞近来在东部排名第二。\\n', '\\t“真是天造地设的一对——我父亲喜欢结交名人，杰姬酷爱金钱，”亚历山大在婚礼上讥讽道。他和克里斯蒂娜从未同他们的继母和睦相处过。\\n', '\\t2006年，沃尔玛的推荐引擎竟将《人猿星球》与马丁·路德·金的记录片配成了一对，为此沃尔玛遭到了种族歧视的指控。\\n', '\\t通过电子探针显微分析确定贫化渣中主要铜相为冰铜相。\\n', '\\t吉姆靠给人擦皮鞋为生。\\n', '\\t用甘氨酸模拟胶原，研究间苯二酚-恶唑烷E鞣性基质的形成以及与胶原之间的反应特性。\\n']\n"
     ]
    }
   ],
   "source": [
    "print(target_texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7774e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典排序            \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d7ad67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算編碼器、解碼器的最大長度\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0cfeacd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 50\n",
      "Number of unique input tokens: 72\n",
      "Number of unique output tokens: 733\n",
      "Max sequence length for inputs: 197\n",
      "Max sequence length for outputs: 70\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4db274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以dict儲存字典單字及序號\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "157a166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定編碼器、解碼器input起始值(均為0矩陣)\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "371b6863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '\"': 1, '&': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, '?': 20, 'A': 21, 'B': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'J': 30, 'K': 31, 'L': 32, 'M': 33, 'N': 34, 'O': 35, 'P': 36, 'R': 37, 'S': 38, 'T': 39, 'U': 40, 'W': 41, 'Y': 42, 'a': 43, 'b': 44, 'c': 45, 'd': 46, 'e': 47, 'f': 48, 'g': 49, 'h': 50, 'i': 51, 'j': 52, 'k': 53, 'l': 54, 'm': 55, 'n': 56, 'o': 57, 'p': 58, 'q': 59, 'r': 60, 's': 61, 't': 62, 'u': 63, 'v': 64, 'w': 65, 'x': 66, 'y': 67, 'z': 68, '—': 69, '“': 70, '…': 71}\n"
     ]
    }
   ],
   "source": [
    "print(input_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "22f5090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 0, '\\n': 1, ' ': 2, '\"': 3, '-': 4, '.': 5, '0': 6, '1': 7, '2': 8, '3': 9, '5': 10, '6': 11, '8': 12, '9': 13, '?': 14, 'A': 15, 'C': 16, 'E': 17, 'I': 18, 'K': 19, 'L': 20, 'O': 21, 'a': 22, 'l': 23, 'm': 24, 'u': 25, '·': 26, '—': 27, '“': 28, '”': 29, '…': 30, '、': 31, '。': 32, '《': 33, '》': 34, '「': 35, '」': 36, '一': 37, '丁': 38, '三': 39, '上': 40, '下': 41, '不': 42, '与': 43, '且': 44, '世': 45, '业': 46, '东': 47, '丝': 48, '两': 49, '严': 50, '个': 51, '中': 52, '丹': 53, '为': 54, '主': 55, '丽': 56, '久': 57, '么': 58, '义': 59, '之': 60, '习': 61, '书': 62, '买': 63, '了': 64, '予': 65, '争': 66, '二': 67, '于': 68, '云': 69, '亚': 70, '些': 71, '亟': 72, '交': 73, '亦': 74, '产': 75, '享': 76, '亮': 77, '亲': 78, '人': 79, '什': 80, '今': 81, '从': 82, '他': 83, '代': 84, '令': 85, '以': 86, '们': 87, '价': 88, '任': 89, '份': 90, '伊': 91, '优': 92, '会': 93, '传': 94, '伤': 95, '伴': 96, '似': 97, '但': 98, '位': 99, '住': 100, '体': 101, '作': 102, '你': 103, '使': 104, '供': 105, '俄': 106, '保': 107, '信': 108, '候': 109, '做': 110, '停': 111, '儿': 112, '先': 113, '克': 114, '免': 115, '公': 116, '兰': 117, '共': 118, '关': 119, '兴': 120, '其': 121, '具': 122, '内': 123, '再': 124, '冬': 125, '冰': 126, '决': 127, '况': 128, '冷': 129, '冻': 130, '准': 131, '出': 132, '击': 133, '分': 134, '则': 135, '利': 136, '别': 137, '刮': 138, '到': 139, '制': 140, '剂': 141, '前': 142, '剧': 143, '剪': 144, '力': 145, '功': 146, '加': 147, '动': 148, '勒': 149, '包': 150, '化': 151, '区': 152, '十': 153, '升': 154, '华': 155, '协': 156, '单': 157, '南': 158, '博': 159, '印': 160, '即': 161, '历': 162, '原': 163, '去': 164, '参': 165, '及': 166, '反': 167, '发': 168, '取': 169, '变': 170, '古': 171, '可': 172, '各': 173, '合': 174, '吉': 175, '同': 176, '名': 177, '后': 178, '吗': 179, '吨': 180, '听': 181, '告': 182, '员': 183, '和': 184, '咱': 185, '品': 186, '哈': 187, '响': 188, '唑': 189, '喜': 190, '器': 191, '回': 192, '因': 193, '园': 194, '国': 195, '在': 196, '地': 197, '场': 198, '均': 199, '块': 200, '垂': 201, '型': 202, '垫': 203, '城': 204, '基': 205, '堂': 206, '塞': 207, '境': 208, '增': 209, '士': 210, '声': 211, '处': 212, '备': 213, '复': 214, '外': 215, '多': 216, '夜': 217, '大': 218, '天': 219, '太': 220, '夫': 221, '奋': 222, '女': 223, '好': 224, '如': 225, '妇': 226, '妻': 227, '姆': 228, '始': 229, '姬': 230, '娜': 231, '婚': 232, '子': 233, '字': 234, '存': 235, '孙': 236, '学': 237, '它': 238, '安': 239, '完': 240, '定': 241, '宝': 242, '实': 243, '宠': 244, '客': 245, '宣': 246, '宫': 247, '家': 248, '容': 249, '寄': 250, '富': 251, '寒': 252, '对': 253, '将': 254, '小': 255, '尔': 256, '尖': 257, '就': 258, '尼': 259, '展': 260, '属': 261, '山': 262, '岁': 263, '州': 264, '工': 265, '己': 266, '已': 267, '巴': 268, '市': 269, '布': 270, '帖': 271, '带': 272, '席': 273, '常': 274, '平': 275, '年': 276, '并': 277, '库': 278, '应': 279, '底': 280, '度': 281, '开': 282, '弃': 283, '式': 284, '引': 285, '弗': 286, '弯': 287, '强': 288, '当': 289, '录': 290, '形': 291, '影': 292, '往': 293, '待': 294, '很': 295, '律': 296, '得': 297, '微': 298, '德': 299, '必': 300, '快': 301, '态': 302, '思': 303, '急': 304, '性': 305, '息': 306, '恶': 307, '患': 308, '情': 309, '想': 310, '意': 311, '感': 312, '愿': 313, '慢': 314, '慰': 315, '成': 316, '我': 317, '或': 318, '战': 319, '户': 320, '所': 321, '才': 322, '打': 323, '扩': 324, '扬': 325, '承': 326, '把': 327, '抗': 328, '抚': 329, '报': 330, '担': 331, '拉': 332, '拟': 333, '拿': 334, '持': 335, '指': 336, '挑': 337, '据': 338, '授': 339, '排': 340, '探': 341, '控': 342, '推': 343, '提': 344, '揭': 345, '擎': 346, '擦': 347, '改': 348, '放': 349, '教': 350, '敢': 351, '散': 352, '数': 353, '整': 354, '文': 355, '斯': 356, '新': 357, '方': 358, '族': 359, '无': 360, '日': 361, '旱': 362, '时': 363, '明': 364, '星': 365, '映': 366, '昨': 367, '是': 368, '显': 369, '晓': 370, '晚': 371, '暖': 372, '曲': 373, '更': 374, '最': 375, '月': 376, '有': 377, '服': 378, '期': 379, '未': 380, '本': 381, '朵': 382, '机': 383, '权': 384, '李': 385, '来': 386, '杯': 387, '杰': 388, '板': 389, '极': 390, '析': 391, '果': 392, '架': 393, '柔': 394, '柠': 395, '查': 396, '标': 397, '校': 398, '样': 399, '核': 400, '根': 401, '格': 402, '案': 403, '档': 404, '检': 405, '模': 406, '檬': 407, '欢': 408, '歌': 409, '正': 410, '此': 411, '步': 412, '歧': 413, '段': 414, '母': 415, '毒': 416, '比': 417, '毛': 418, '氨': 419, '水': 420, '永': 421, '汁': 422, '求': 423, '汤': 424, '汰': 425, '汽': 426, '沃': 427, '没': 428, '法': 429, '波': 430, '注': 431, '洗': 432, '活': 433, '流': 434, '浊': 435, '测': 436, '济': 437, '浏': 438, '浓': 439, '浴': 440, '海': 441, '涉': 442, '液': 443, '涵': 444, '淘': 445, '混': 446, '渣': 447, '温': 448, '游': 449, '湖': 450, '源': 451, '溜': 452, '滋': 453, '演': 454, '漫': 455, '澡': 456, '火': 457, '点': 458, '烈': 459, '热': 460, '烷': 461, '然': 462, '爪': 463, '爱': 464, '父': 465, '片': 466, '版': 467, '物': 468, '特': 469, '狗': 470, '独': 471, '狱': 472, '猿': 473, '玛': 474, '环': 475, '现': 476, '球': 477, '理': 478, '甘': 479, '生': 480, '用': 481, '由': 482, '电': 483, '男': 484, '界': 485, '留': 486, '疑': 487, '疼': 488, '病': 489, '痛': 490, '癌': 491, '白': 492, '的': 493, '皮': 494, '益': 495, '监': 496, '盛': 497, '盟': 498, '目': 499, '直': 500, '相': 501, '盾': 502, '看': 503, '真': 504, '着': 505, '睦': 506, '矛': 507, '知': 508, '石': 509, '码': 510, '研': 511, '破': 512, '确': 513, '示': 514, '礼': 515, '社': 516, '神': 517, '福': 518, '秀': 519, '种': 520, '称': 521, '稀': 522, '程': 523, '稳': 524, '稿': 525, '究': 526, '空': 527, '穿': 528, '突': 529, '立': 530, '站': 531, '竟': 532, '章': 533, '符': 534, '第': 535, '等': 536, '答': 537, '算': 538, '管': 539, '箱': 540, '篮': 541, '类': 542, '粒': 543, '精': 544, '红': 545, '级': 546, '纯': 547, '组': 548, '细': 549, '经': 550, '结': 551, '给': 552, '统': 553, '继': 554, '续': 555, '维': 556, '绿': 557, '缓': 558, '编': 559, '网': 560, '罗': 561, '美': 562, '羞': 563, '群': 564, '考': 565, '者': 566, '而': 567, '耳': 568, '联': 569, '育': 570, '胜': 571, '胞': 572, '胶': 573, '能': 574, '脚': 575, '腹': 576, '自': 577, '至': 578, '舒': 579, '艾': 580, '节': 581, '花': 582, '苓': 583, '苯': 584, '茯': 585, '荐': 586, '药': 587, '莉': 588, '莎': 589, '获': 590, '蒂': 591, '藏': 592, '虑': 593, '血': 594, '行': 595, '表': 596, '衰': 597, '袜': 598, '被': 599, '装': 600, '要': 601, '见': 602, '规': 603, '视': 604, '览': 605, '解': 606, '言': 607, '认': 608, '讥': 609, '让': 610, '记': 611, '许': 612, '讽': 613, '设': 614, '证': 615, '识': 616, '译': 617, '语': 618, '说': 619, '谁': 620, '调': 621, '谓': 622, '谢': 623, '负': 624, '责': 625, '货': 626, '质': 627, '贫': 628, '贵': 629, '费': 630, '资': 631, '赢': 632, '赤': 633, '起': 634, '越': 635, '足': 636, '跨': 637, '路': 638, '跳': 639, '身': 640, '躯': 641, '车': 642, '软': 643, '载': 644, '边': 645, '达': 646, '过': 647, '运': 648, '近': 649, '还': 650, '这': 651, '进': 652, '远': 653, '迷': 654, '退': 655, '送': 656, '适': 657, '逐': 658, '通': 659, '造': 660, '道': 661, '遗': 662, '遭': 663, '避': 664, '邓': 665, '那': 666, '邮': 667, '部': 668, '都': 669, '配': 670, '酚': 671, '酷': 672, '酸': 673, '释': 674, '里': 675, '重': 676, '量': 677, '金': 678, '针': 679, '钢': 680, '钱': 681, '铜': 682, '铭': 683, '银': 684, '销': 685, '锐': 686, '长': 687, '问': 688, '间': 689, '队': 690, '阶': 691, '阿': 692, '附': 693, '际': 694, '降': 695, '限': 696, '院': 697, '陪': 698, '随': 699, '隐': 700, '障': 701, '难': 702, '集': 703, '需': 704, '非': 705, '靠': 706, '面': 707, '鞋': 708, '鞣': 709, '韩': 710, '音': 711, '须': 712, '顿': 713, '颈': 714, '颗': 715, '题': 716, '风': 717, '飞': 718, '饰': 719, '首': 720, '马': 721, '高': 722, '鲜': 723, '麦': 724, '鼓': 725, '龄': 726, '（': 727, '）': 728, '，': 729, '：': 730, '；': 731, '？': 732}\n"
     ]
    }
   ],
   "source": [
    "print(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a8a9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 encoder_input、decoder_input對應的順序    \n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56e36a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='lstm_4/PartitionedCall:0', description=\"created by layer 'lstm_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='lstm_4/PartitionedCall:2', description=\"created by layer 'lstm_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='lstm_4/PartitionedCall:3', description=\"created by layer 'lstm_4'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(None, 256), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立 encoder LSTM 隱藏層\n",
    "origin_encoder_inputs = Input(shape=(None, num_encoder_tokens), name='input_0')\n",
    "origin_encoder = LSTM(latent_dim, return_state=True)\n",
    "origin_encoder_outputs, origin_state_h, origin_state_c = origin_encoder(origin_encoder_inputs)\n",
    "origin_encoder_states = [origin_state_h, origin_state_c]\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='input_1')\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "print(encoder_outputs)\n",
    "print(state_h)\n",
    "print(state_c)\n",
    "encoder_outputs.type_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d31d97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 捨棄 output，只保留記憶狀態 h 及 c\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebdacc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 decoder LSTM 隱藏層\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='input_2')\n",
    "# We set up our decoder to return full output sequences,\n",
    "# decoder 記憶狀態不會在訓練過程使用，只會在推論(Inference)使用\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "55f7a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義模型，由 encoder_input_data 及 decoder_input_data 轉換為 decoder_target_data \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96c65a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 19s 19s/step - loss: 3.1431 - val_loss: 3.9181\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.1342 - val_loss: 3.9125\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1222 - val_loss: 3.8097\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.9456 - val_loss: 3.8037\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8767 - val_loss: 3.7869\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8574 - val_loss: 3.8119\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8422 - val_loss: 3.7987\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8302 - val_loss: 3.8198\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8341 - val_loss: 3.8100\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8167 - val_loss: 3.8269\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8199 - val_loss: 3.8114\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8103 - val_loss: 3.8319\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8232 - val_loss: 3.8201\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8026 - val_loss: 3.8266\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7974 - val_loss: 3.8192\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7924 - val_loss: 3.8313\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7970 - val_loss: 3.8104\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7972 - val_loss: 3.8362\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8188 - val_loss: 3.8298\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7908 - val_loss: 3.8323\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7842 - val_loss: 3.8297\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7836 - val_loss: 3.8274\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7771 - val_loss: 3.8349\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7864 - val_loss: 3.8251\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7743 - val_loss: 3.8364\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7941 - val_loss: 3.8253\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7715 - val_loss: 3.8371\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7746 - val_loss: 3.8311\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7654 - val_loss: 3.8372\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7785 - val_loss: 3.8349\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7624 - val_loss: 3.8365\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7711 - val_loss: 3.8306\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7578 - val_loss: 3.8397\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7795 - val_loss: 3.8337\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7583 - val_loss: 3.8400\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7574 - val_loss: 3.8343\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7501 - val_loss: 3.8398\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7646 - val_loss: 3.8353\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7495 - val_loss: 3.8377\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7724 - val_loss: 3.8380\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7460 - val_loss: 3.8368\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7497 - val_loss: 3.8352\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7421 - val_loss: 3.8404\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7619 - val_loss: 3.8369\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7393 - val_loss: 3.8428\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7598 - val_loss: 3.8376\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7379 - val_loss: 3.8380\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7555 - val_loss: 3.8347\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7383 - val_loss: 3.8416\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7510 - val_loss: 3.8363\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7332 - val_loss: 3.8402\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7558 - val_loss: 3.8386\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7333 - val_loss: 3.8397\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7424 - val_loss: 3.8365\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7278 - val_loss: 3.8449\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7555 - val_loss: 3.8366\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7338 - val_loss: 3.8425\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7370 - val_loss: 3.8406\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7270 - val_loss: 3.8370\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7422 - val_loss: 3.8361\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7222 - val_loss: 3.8411\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7366 - val_loss: 3.8373\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7212 - val_loss: 3.8432\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7541 - val_loss: 3.8372\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7270 - val_loss: 3.8384\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7253 - val_loss: 3.8367\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7167 - val_loss: 3.8360\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7384 - val_loss: 3.8377\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7162 - val_loss: 3.8338\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7140 - val_loss: 3.8418\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7229 - val_loss: 3.8343\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7224 - val_loss: 3.8459\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7531 - val_loss: 3.8416\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7227 - val_loss: 3.8444\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7221 - val_loss: 3.8461\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7141 - val_loss: 3.8410\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7373 - val_loss: 3.8396\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7152 - val_loss: 3.8404\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7088 - val_loss: 3.8412\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7036 - val_loss: 3.8410\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7126 - val_loss: 3.8357\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7030 - val_loss: 3.8382\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7307 - val_loss: 3.8347\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7063 - val_loss: 3.8394\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 2.7114 - val_loss: 3.8358\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6971 - val_loss: 3.8369\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7039 - val_loss: 3.8348\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6916 - val_loss: 3.8391\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7131 - val_loss: 3.8362\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6908 - val_loss: 3.8319\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7154 - val_loss: 3.8381\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6879 - val_loss: 3.8378\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7010 - val_loss: 3.8360\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6854 - val_loss: 3.8380\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7114 - val_loss: 3.8337\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6863 - val_loss: 3.8424\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7124 - val_loss: 3.8357\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6886 - val_loss: 3.8372\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7058 - val_loss: 3.8355\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6812 - val_loss: 3.8380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a64dd85220>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a2ba5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義編碼器取樣模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7abca2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義解碼器的input\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b132fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義解碼器 LSTM 模型\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aefe4c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(None, None, 733) dtype=float32 (created by layer 'input_2')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'input_3')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'input_4')>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndecoder_model = Model(inputs=[decoder_inputs].append(decoder_states_inputs), outputs=[decoder_outputs].append(decoder_states))\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以編碼器的記憶狀態 h 及 c 為解碼器的記憶狀態  \n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "print([decoder_inputs] + decoder_states_inputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "'''\n",
    "decoder_model = Model(inputs=[decoder_inputs].append(decoder_states_inputs), outputs=[decoder_outputs].append(decoder_states))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c0f6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立反向的 dict，才能透過查詢將數值轉回文字\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d12c3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型預測，並取得翻譯結果(中文)    \n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c78daa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He had formerly been in business at Bristol, but failed in debt to a number of people, compounded, and went to America.', 'Basing on the on site tests of anchor, authors found that anchors have obvious pre stress loss problem during stretching and locking, analyzed and proposed several solutions.', 'From hair tip first began gradually, after all, through from downward, nodular comb.', 'The sky began to be clear up a bit when we left St Gallen abbey and library.', \"Once more, Cinderella's fairy godmother reminded her to be home by midnight.\", 'This paper introduces the demand analysis and function design in detail, gives the source codes of relevant interface functions and base algorithm.', 'Manufacturer of thin and ultra-thin non-ferrous metal foils mainly made of copper, copper-alloys, nickel, genuine silver and nickel-silver.', \"All day thy wings have fann'd At that far height, the cold thin atmosphere: Yet stoop not, weary, to the welcome land, Though the dark night is near.\", 'The two other attackers are believed to have tried to enter the terminal, which is protected by heavily armed police and X-ray machines.', 'He became in legitimately through the door of the Law (vv. 1-3).']\n",
      "{'T', 's', 'd', 'y', 'w', 'S', ':', 'F', ',', 'B', 't', 'Y', 'i', \"'\", 'k', '(', 'p', 'n', 'f', 'h', 'e', 'g', 'H', 'M', 'z', 'b', ')', 'X', 'r', 'v', 'c', 'G', 'o'}\n",
      "[' ', '\"', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '“', '…']\n"
     ]
    }
   ],
   "source": [
    "test_texts = []\n",
    "test_characters = set()\n",
    "for text in test_data:\n",
    "    test_texts.append(text['english'])\n",
    "    for char in text['english']:\n",
    "        if char not in target_characters:\n",
    "            test_characters.add(char)\n",
    "\n",
    "print(test_texts)\n",
    "print(test_characters)\n",
    "print(input_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 encoder_input、decoder_input對應的順序    \n",
    "for i, test_text in enumerate(test_texts):\n",
    "    for t, char in enumerate(test_text):\n",
    "        encoder_input_data[i, t, test_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9376f81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "Input sentence: For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: He calls the Green Book, his book of teachings, “the new gospel.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: And the light breeze moves me to caress her long ear\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: They have the blood of martyrs is the White to flow …\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: Finally, the Lakers head to the Motor City to take on a Pistons team that currently owns the Eastern Conference's second best record (1/31). L.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: \"The perfect match—my father loves names and Jackie loves money, \" sneered Alexander at the wedding. Neither he nor Christina ever got along with their stepmother17.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: In 2006, Walmart was charged with racism when its recommendation engine paired Planet of the Apes with a documentary about Martin Luther King.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: The matte as main copper phase in the cleaning. slag was deter- mined by electron probe microscopic analysis.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: Have you shined your shoes?\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: The Tanning Matrix can be formed by resorcinol and oxazolidine E, and the reactioncharateristics between Tanning Matrix and collagen were investigated through NMR and size distribution analysis.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: Free delivery for addresses in the city. Can be delivered through Internet.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: Keele University is renowned for its exciting approach to higher education, beautiful campus, strong community spirit and excellent student life.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: Among them, there was the herb Tuckahoe grown in Yunnan and Guizhou provinces.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: Your willingness to sacrifice countless late nights consoling them?\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "*\n",
      "Input sentence: Callum: OK, we'll find out if you're right at the end of the programme.\n",
      "Decoded sentence: 这人的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-d7efc691f1a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# for trying out decoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input sentence:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-2bcecea93a48>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         output_tokens, h, c = decoder_model.predict(\n\u001b[0m\u001b[0;32m     17\u001b[0m             [target_seq] + states_value)\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1669\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1671\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1672\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1346\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     dataset = dataset.map(\n\u001b[0m\u001b[0;32m    356\u001b[0m         grab_batch, num_parallel_calls=tf.data.AUTOTUNE)\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1925\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1927\u001b[1;33m       return ParallelMapDataset(\n\u001b[0m\u001b[0;32m   1928\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1929\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4520\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4521\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4522\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4523\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4524\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3710\u001b[0m     \u001b[0mresource_tracker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3711\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3712\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3713\u001b[0m       \u001b[1;31m# There is no graph to add in eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3714\u001b[0m       \u001b[0madd_to_graph\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3132\u001b[0m          \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m     \"\"\"\n\u001b[1;32m-> 3134\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   3135\u001b[0m         *args, **kwargs)\n\u001b[0;32m   3136\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3098\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3099\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3100\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3101\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3102\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3276\u001b[0m     ]\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3278\u001b[1;33m     graph_function = ConcreteFunction(\n\u001b[0m\u001b[0;32m   3279\u001b[0m         func_graph_module.func_graph_from_py_func(\n\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[0;32m   1595\u001b[0m     \u001b[1;31m# These each get a reference to the FuncGraph deleter since they use the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1596\u001b[0m     \u001b[1;31m# FuncGraph directly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1597\u001b[1;33m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0m\u001b[0;32m   1598\u001b[0m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0;32m   1599\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m     self._inference_function = _EagerDefinedFunction(\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m       \u001b[0moutput_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m     fn = pywrap_tf_session.TF_GraphToFunction_wrapper(\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 測試100次\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('*')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    try:\n",
    "        print('Decoded sentence:', decoded_sentence)\n",
    "    except:\n",
    "        # 出現亂碼，以?取代\n",
    "        print('Decoded sentence:', decoded_sentence.encode('ascii', 'replace'))\n",
    "        #print(\"error:\", sys.exc_info()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c324df7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder_input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918fc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
