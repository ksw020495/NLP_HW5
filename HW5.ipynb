{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b8973a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12741023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninput_file = open('translation2019zh_valid.json', encoding = 'utf8')\\nfor line in input_file.readlines():\\n    num = num + 1\\n    #data.append(json.loads(line))\\ninput_file.close()\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#讀檔\n",
    "\n",
    "num = 0\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_size = 500\n",
    "test_size = 10\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "input_file = open('translation2019zh_train.json', encoding = 'utf8')\n",
    "for line in input_file.readlines():\n",
    "    num = num + 1\n",
    "    if num > train_size + test_size:\n",
    "        break\n",
    "    elif num > train_size:\n",
    "        test_data.append(json.loads(line))\n",
    "    else:\n",
    "        train_data.append(json.loads(line))\n",
    "input_file.close()\n",
    "\n",
    "'''\n",
    "input_file = open('translation2019zh_valid.json', encoding = 'utf8')\n",
    "for line in input_file.readlines():\n",
    "    num = num + 1\n",
    "    #data.append(json.loads(line))\n",
    "input_file.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1738b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cc7152ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#簡轉繁\\nfrom opencc import OpenCC\\n\\ncc = OpenCC('s2t')\\n\\nfor data in train_data:\\n    data['chinese'] = cc.convert(data['chinese'])\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#簡轉繁\n",
    "from opencc import OpenCC\n",
    "\n",
    "cc = OpenCC('s2t')\n",
    "\n",
    "for data in train_data:\n",
    "    data['chinese'] = cc.convert(data['chinese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d0c01d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料檔，並將所有單字整理為字典，分別為英文及中文字典，注意，英文為字母的集合，非單字(Word)\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for item in train_data:\n",
    "    input_text = item['english']\n",
    "    target_text = '\\t' + item['chinese'] + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fe0b0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。\\n', '\\t他还把宣扬自己思想的所谓《绿皮书》称作“新福音书”。\\n', '\\t微风推着我去爱抚它的长耳朵\\n', '\\t它们的先烈们的鲜血是白流了…\\n', '\\t最后，在1月31日，湖人将前往汽车城底特律挑战活塞队，活塞近来在东部排名第二。\\n', '\\t“真是天造地设的一对——我父亲喜欢结交名人，杰姬酷爱金钱，”亚历山大在婚礼上讥讽道。他和克里斯蒂娜从未同他们的继母和睦相处过。\\n', '\\t2006年，沃尔玛的推荐引擎竟将《人猿星球》与马丁·路德·金的记录片配成了一对，为此沃尔玛遭到了种族歧视的指控。\\n', '\\t通过电子探针显微分析确定贫化渣中主要铜相为冰铜相。\\n', '\\t吉姆靠给人擦皮鞋为生。\\n', '\\t用甘氨酸模拟胶原，研究间苯二酚-恶唑烷E鞣性基质的形成以及与胶原之间的反应特性。\\n']\n"
     ]
    }
   ],
   "source": [
    "print(target_texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7774e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典排序            \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d7ad67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算編碼器、解碼器的最大長度\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0cfeacd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 50\n",
      "Number of unique input tokens: 72\n",
      "Number of unique output tokens: 733\n",
      "Max sequence length for inputs: 197\n",
      "Max sequence length for outputs: 70\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4db274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以dict儲存字典單字及序號\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "157a166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定編碼器、解碼器input起始值(均為0矩陣)\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "add83ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '\"': 1, '&': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, '?': 20, 'A': 21, 'B': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'J': 30, 'K': 31, 'L': 32, 'M': 33, 'N': 34, 'O': 35, 'P': 36, 'R': 37, 'S': 38, 'T': 39, 'U': 40, 'W': 41, 'Y': 42, 'a': 43, 'b': 44, 'c': 45, 'd': 46, 'e': 47, 'f': 48, 'g': 49, 'h': 50, 'i': 51, 'j': 52, 'k': 53, 'l': 54, 'm': 55, 'n': 56, 'o': 57, 'p': 58, 'q': 59, 'r': 60, 's': 61, 't': 62, 'u': 63, 'v': 64, 'w': 65, 'x': 66, 'y': 67, 'z': 68, '—': 69, '“': 70, '…': 71}\n"
     ]
    }
   ],
   "source": [
    "print(input_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8440b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 0, '\\n': 1, ' ': 2, '\"': 3, '-': 4, '.': 5, '0': 6, '1': 7, '2': 8, '3': 9, '5': 10, '6': 11, '8': 12, '9': 13, '?': 14, 'A': 15, 'C': 16, 'E': 17, 'I': 18, 'K': 19, 'L': 20, 'O': 21, 'a': 22, 'l': 23, 'm': 24, 'u': 25, '·': 26, '—': 27, '“': 28, '”': 29, '…': 30, '、': 31, '。': 32, '《': 33, '》': 34, '「': 35, '」': 36, '一': 37, '丁': 38, '三': 39, '上': 40, '下': 41, '不': 42, '与': 43, '且': 44, '世': 45, '业': 46, '东': 47, '丝': 48, '两': 49, '严': 50, '个': 51, '中': 52, '丹': 53, '为': 54, '主': 55, '丽': 56, '久': 57, '么': 58, '义': 59, '之': 60, '习': 61, '书': 62, '买': 63, '了': 64, '予': 65, '争': 66, '二': 67, '于': 68, '云': 69, '亚': 70, '些': 71, '亟': 72, '交': 73, '亦': 74, '产': 75, '享': 76, '亮': 77, '亲': 78, '人': 79, '什': 80, '今': 81, '从': 82, '他': 83, '代': 84, '令': 85, '以': 86, '们': 87, '价': 88, '任': 89, '份': 90, '伊': 91, '优': 92, '会': 93, '传': 94, '伤': 95, '伴': 96, '似': 97, '但': 98, '位': 99, '住': 100, '体': 101, '作': 102, '你': 103, '使': 104, '供': 105, '俄': 106, '保': 107, '信': 108, '候': 109, '做': 110, '停': 111, '儿': 112, '先': 113, '克': 114, '免': 115, '公': 116, '兰': 117, '共': 118, '关': 119, '兴': 120, '其': 121, '具': 122, '内': 123, '再': 124, '冬': 125, '冰': 126, '决': 127, '况': 128, '冷': 129, '冻': 130, '准': 131, '出': 132, '击': 133, '分': 134, '则': 135, '利': 136, '别': 137, '刮': 138, '到': 139, '制': 140, '剂': 141, '前': 142, '剧': 143, '剪': 144, '力': 145, '功': 146, '加': 147, '动': 148, '勒': 149, '包': 150, '化': 151, '区': 152, '十': 153, '升': 154, '华': 155, '协': 156, '单': 157, '南': 158, '博': 159, '印': 160, '即': 161, '历': 162, '原': 163, '去': 164, '参': 165, '及': 166, '反': 167, '发': 168, '取': 169, '变': 170, '古': 171, '可': 172, '各': 173, '合': 174, '吉': 175, '同': 176, '名': 177, '后': 178, '吗': 179, '吨': 180, '听': 181, '告': 182, '员': 183, '和': 184, '咱': 185, '品': 186, '哈': 187, '响': 188, '唑': 189, '喜': 190, '器': 191, '回': 192, '因': 193, '园': 194, '国': 195, '在': 196, '地': 197, '场': 198, '均': 199, '块': 200, '垂': 201, '型': 202, '垫': 203, '城': 204, '基': 205, '堂': 206, '塞': 207, '境': 208, '增': 209, '士': 210, '声': 211, '处': 212, '备': 213, '复': 214, '外': 215, '多': 216, '夜': 217, '大': 218, '天': 219, '太': 220, '夫': 221, '奋': 222, '女': 223, '好': 224, '如': 225, '妇': 226, '妻': 227, '姆': 228, '始': 229, '姬': 230, '娜': 231, '婚': 232, '子': 233, '字': 234, '存': 235, '孙': 236, '学': 237, '它': 238, '安': 239, '完': 240, '定': 241, '宝': 242, '实': 243, '宠': 244, '客': 245, '宣': 246, '宫': 247, '家': 248, '容': 249, '寄': 250, '富': 251, '寒': 252, '对': 253, '将': 254, '小': 255, '尔': 256, '尖': 257, '就': 258, '尼': 259, '展': 260, '属': 261, '山': 262, '岁': 263, '州': 264, '工': 265, '己': 266, '已': 267, '巴': 268, '市': 269, '布': 270, '帖': 271, '带': 272, '席': 273, '常': 274, '平': 275, '年': 276, '并': 277, '库': 278, '应': 279, '底': 280, '度': 281, '开': 282, '弃': 283, '式': 284, '引': 285, '弗': 286, '弯': 287, '强': 288, '当': 289, '录': 290, '形': 291, '影': 292, '往': 293, '待': 294, '很': 295, '律': 296, '得': 297, '微': 298, '德': 299, '必': 300, '快': 301, '态': 302, '思': 303, '急': 304, '性': 305, '息': 306, '恶': 307, '患': 308, '情': 309, '想': 310, '意': 311, '感': 312, '愿': 313, '慢': 314, '慰': 315, '成': 316, '我': 317, '或': 318, '战': 319, '户': 320, '所': 321, '才': 322, '打': 323, '扩': 324, '扬': 325, '承': 326, '把': 327, '抗': 328, '抚': 329, '报': 330, '担': 331, '拉': 332, '拟': 333, '拿': 334, '持': 335, '指': 336, '挑': 337, '据': 338, '授': 339, '排': 340, '探': 341, '控': 342, '推': 343, '提': 344, '揭': 345, '擎': 346, '擦': 347, '改': 348, '放': 349, '教': 350, '敢': 351, '散': 352, '数': 353, '整': 354, '文': 355, '斯': 356, '新': 357, '方': 358, '族': 359, '无': 360, '日': 361, '旱': 362, '时': 363, '明': 364, '星': 365, '映': 366, '昨': 367, '是': 368, '显': 369, '晓': 370, '晚': 371, '暖': 372, '曲': 373, '更': 374, '最': 375, '月': 376, '有': 377, '服': 378, '期': 379, '未': 380, '本': 381, '朵': 382, '机': 383, '权': 384, '李': 385, '来': 386, '杯': 387, '杰': 388, '板': 389, '极': 390, '析': 391, '果': 392, '架': 393, '柔': 394, '柠': 395, '查': 396, '标': 397, '校': 398, '样': 399, '核': 400, '根': 401, '格': 402, '案': 403, '档': 404, '检': 405, '模': 406, '檬': 407, '欢': 408, '歌': 409, '正': 410, '此': 411, '步': 412, '歧': 413, '段': 414, '母': 415, '毒': 416, '比': 417, '毛': 418, '氨': 419, '水': 420, '永': 421, '汁': 422, '求': 423, '汤': 424, '汰': 425, '汽': 426, '沃': 427, '没': 428, '法': 429, '波': 430, '注': 431, '洗': 432, '活': 433, '流': 434, '浊': 435, '测': 436, '济': 437, '浏': 438, '浓': 439, '浴': 440, '海': 441, '涉': 442, '液': 443, '涵': 444, '淘': 445, '混': 446, '渣': 447, '温': 448, '游': 449, '湖': 450, '源': 451, '溜': 452, '滋': 453, '演': 454, '漫': 455, '澡': 456, '火': 457, '点': 458, '烈': 459, '热': 460, '烷': 461, '然': 462, '爪': 463, '爱': 464, '父': 465, '片': 466, '版': 467, '物': 468, '特': 469, '狗': 470, '独': 471, '狱': 472, '猿': 473, '玛': 474, '环': 475, '现': 476, '球': 477, '理': 478, '甘': 479, '生': 480, '用': 481, '由': 482, '电': 483, '男': 484, '界': 485, '留': 486, '疑': 487, '疼': 488, '病': 489, '痛': 490, '癌': 491, '白': 492, '的': 493, '皮': 494, '益': 495, '监': 496, '盛': 497, '盟': 498, '目': 499, '直': 500, '相': 501, '盾': 502, '看': 503, '真': 504, '着': 505, '睦': 506, '矛': 507, '知': 508, '石': 509, '码': 510, '研': 511, '破': 512, '确': 513, '示': 514, '礼': 515, '社': 516, '神': 517, '福': 518, '秀': 519, '种': 520, '称': 521, '稀': 522, '程': 523, '稳': 524, '稿': 525, '究': 526, '空': 527, '穿': 528, '突': 529, '立': 530, '站': 531, '竟': 532, '章': 533, '符': 534, '第': 535, '等': 536, '答': 537, '算': 538, '管': 539, '箱': 540, '篮': 541, '类': 542, '粒': 543, '精': 544, '红': 545, '级': 546, '纯': 547, '组': 548, '细': 549, '经': 550, '结': 551, '给': 552, '统': 553, '继': 554, '续': 555, '维': 556, '绿': 557, '缓': 558, '编': 559, '网': 560, '罗': 561, '美': 562, '羞': 563, '群': 564, '考': 565, '者': 566, '而': 567, '耳': 568, '联': 569, '育': 570, '胜': 571, '胞': 572, '胶': 573, '能': 574, '脚': 575, '腹': 576, '自': 577, '至': 578, '舒': 579, '艾': 580, '节': 581, '花': 582, '苓': 583, '苯': 584, '茯': 585, '荐': 586, '药': 587, '莉': 588, '莎': 589, '获': 590, '蒂': 591, '藏': 592, '虑': 593, '血': 594, '行': 595, '表': 596, '衰': 597, '袜': 598, '被': 599, '装': 600, '要': 601, '见': 602, '规': 603, '视': 604, '览': 605, '解': 606, '言': 607, '认': 608, '讥': 609, '让': 610, '记': 611, '许': 612, '讽': 613, '设': 614, '证': 615, '识': 616, '译': 617, '语': 618, '说': 619, '谁': 620, '调': 621, '谓': 622, '谢': 623, '负': 624, '责': 625, '货': 626, '质': 627, '贫': 628, '贵': 629, '费': 630, '资': 631, '赢': 632, '赤': 633, '起': 634, '越': 635, '足': 636, '跨': 637, '路': 638, '跳': 639, '身': 640, '躯': 641, '车': 642, '软': 643, '载': 644, '边': 645, '达': 646, '过': 647, '运': 648, '近': 649, '还': 650, '这': 651, '进': 652, '远': 653, '迷': 654, '退': 655, '送': 656, '适': 657, '逐': 658, '通': 659, '造': 660, '道': 661, '遗': 662, '遭': 663, '避': 664, '邓': 665, '那': 666, '邮': 667, '部': 668, '都': 669, '配': 670, '酚': 671, '酷': 672, '酸': 673, '释': 674, '里': 675, '重': 676, '量': 677, '金': 678, '针': 679, '钢': 680, '钱': 681, '铜': 682, '铭': 683, '银': 684, '销': 685, '锐': 686, '长': 687, '问': 688, '间': 689, '队': 690, '阶': 691, '阿': 692, '附': 693, '际': 694, '降': 695, '限': 696, '院': 697, '陪': 698, '随': 699, '隐': 700, '障': 701, '难': 702, '集': 703, '需': 704, '非': 705, '靠': 706, '面': 707, '鞋': 708, '鞣': 709, '韩': 710, '音': 711, '须': 712, '顿': 713, '颈': 714, '颗': 715, '题': 716, '风': 717, '飞': 718, '饰': 719, '首': 720, '马': 721, '高': 722, '鲜': 723, '麦': 724, '鼓': 725, '龄': 726, '（': 727, '）': 728, '，': 729, '：': 730, '；': 731, '？': 732}\n"
     ]
    }
   ],
   "source": [
    "print(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a8a9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 encoder_input、decoder_input對應的順序    \n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56e36a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='lstm_4/PartitionedCall:0', description=\"created by layer 'lstm_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='lstm_4/PartitionedCall:2', description=\"created by layer 'lstm_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='lstm_4/PartitionedCall:3', description=\"created by layer 'lstm_4'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(None, 256), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立 encoder LSTM 隱藏層\n",
    "origin_encoder_inputs = Input(shape=(None, num_encoder_tokens), name='input_0')\n",
    "origin_encoder = LSTM(latent_dim, return_state=True)\n",
    "origin_encoder_outputs, origin_state_h, origin_state_c = origin_encoder(origin_encoder_inputs)\n",
    "origin_encoder_states = [origin_state_h, origin_state_c]\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='input_1')\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "print(encoder_outputs)\n",
    "print(state_h)\n",
    "print(state_c)\n",
    "encoder_outputs.type_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d31d97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 捨棄 output，只保留記憶狀態 h 及 c\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebdacc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 decoder LSTM 隱藏層\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='input_2')\n",
    "# We set up our decoder to return full output sequences,\n",
    "# decoder 記憶狀態不會在訓練過程使用，只會在推論(Inference)使用\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "55f7a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義模型，由 encoder_input_data 及 decoder_input_data 轉換為 decoder_target_data \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96c65a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 19s 19s/step - loss: 3.1431 - val_loss: 3.9181\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.1342 - val_loss: 3.9125\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.1222 - val_loss: 3.8097\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.9456 - val_loss: 3.8037\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8767 - val_loss: 3.7869\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8574 - val_loss: 3.8119\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8422 - val_loss: 3.7987\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8302 - val_loss: 3.8198\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8341 - val_loss: 3.8100\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8167 - val_loss: 3.8269\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8199 - val_loss: 3.8114\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8103 - val_loss: 3.8319\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8232 - val_loss: 3.8201\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8026 - val_loss: 3.8266\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7974 - val_loss: 3.8192\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7924 - val_loss: 3.8313\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7970 - val_loss: 3.8104\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7972 - val_loss: 3.8362\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8188 - val_loss: 3.8298\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7908 - val_loss: 3.8323\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7842 - val_loss: 3.8297\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7836 - val_loss: 3.8274\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7771 - val_loss: 3.8349\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7864 - val_loss: 3.8251\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7743 - val_loss: 3.8364\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7941 - val_loss: 3.8253\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7715 - val_loss: 3.8371\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7746 - val_loss: 3.8311\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7654 - val_loss: 3.8372\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7785 - val_loss: 3.8349\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7624 - val_loss: 3.8365\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7711 - val_loss: 3.8306\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7578 - val_loss: 3.8397\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7795 - val_loss: 3.8337\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7583 - val_loss: 3.8400\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7574 - val_loss: 3.8343\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7501 - val_loss: 3.8398\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7646 - val_loss: 3.8353\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7495 - val_loss: 3.8377\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7724 - val_loss: 3.8380\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7460 - val_loss: 3.8368\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7497 - val_loss: 3.8352\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7421 - val_loss: 3.8404\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7619 - val_loss: 3.8369\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7393 - val_loss: 3.8428\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7598 - val_loss: 3.8376\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7379 - val_loss: 3.8380\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7555 - val_loss: 3.8347\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7383 - val_loss: 3.8416\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7510 - val_loss: 3.8363\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7332 - val_loss: 3.8402\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7558 - val_loss: 3.8386\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7333 - val_loss: 3.8397\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7424 - val_loss: 3.8365\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7278 - val_loss: 3.8449\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7555 - val_loss: 3.8366\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7338 - val_loss: 3.8425\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7370 - val_loss: 3.8406\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7270 - val_loss: 3.8370\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7422 - val_loss: 3.8361\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7222 - val_loss: 3.8411\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7366 - val_loss: 3.8373\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7212 - val_loss: 3.8432\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7541 - val_loss: 3.8372\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7270 - val_loss: 3.8384\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7253 - val_loss: 3.8367\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7167 - val_loss: 3.8360\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7384 - val_loss: 3.8377\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7162 - val_loss: 3.8338\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7140 - val_loss: 3.8418\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7229 - val_loss: 3.8343\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7224 - val_loss: 3.8459\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7531 - val_loss: 3.8416\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7227 - val_loss: 3.8444\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7221 - val_loss: 3.8461\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7141 - val_loss: 3.8410\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7373 - val_loss: 3.8396\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7152 - val_loss: 3.8404\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7088 - val_loss: 3.8412\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7036 - val_loss: 3.8410\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.7126 - val_loss: 3.8357\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7030 - val_loss: 3.8382\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7307 - val_loss: 3.8347\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7063 - val_loss: 3.8394\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 2.7114 - val_loss: 3.8358\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6971 - val_loss: 3.8369\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7039 - val_loss: 3.8348\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6916 - val_loss: 3.8391\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7131 - val_loss: 3.8362\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6908 - val_loss: 3.8319\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7154 - val_loss: 3.8381\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6879 - val_loss: 3.8378\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7010 - val_loss: 3.8360\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6854 - val_loss: 3.8380\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7114 - val_loss: 3.8337\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6863 - val_loss: 3.8424\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7124 - val_loss: 3.8357\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6886 - val_loss: 3.8372\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7058 - val_loss: 3.8355\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6812 - val_loss: 3.8380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a64dd85220>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a2ba5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義編碼器取樣模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7abca2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義解碼器的input\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b132fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義解碼器 LSTM 模型\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aefe4c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(None, None, 733) dtype=float32 (created by layer 'input_2')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'input_3')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'input_4')>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndecoder_model = Model(inputs=[decoder_inputs].append(decoder_states_inputs), outputs=[decoder_outputs].append(decoder_states))\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以編碼器的記憶狀態 h 及 c 為解碼器的記憶狀態  \n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "print([decoder_inputs] + decoder_states_inputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "'''\n",
    "decoder_model = Model(inputs=[decoder_inputs].append(decoder_states_inputs), outputs=[decoder_outputs].append(decoder_states))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c0f6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立反向的 dict，才能透過查詢將數值轉回文字\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d12c3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型預測，並取得翻譯結果(中文)    \n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7a3a7690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He had formerly been in business at Bristol, but failed in debt to a number of people, compounded, and went to America.', 'Basing on the on site tests of anchor, authors found that anchors have obvious pre stress loss problem during stretching and locking, analyzed and proposed several solutions.', 'From hair tip first began gradually, after all, through from downward, nodular comb.', 'The sky began to be clear up a bit when we left St Gallen abbey and library.', \"Once more, Cinderella's fairy godmother reminded her to be home by midnight.\", 'This paper introduces the demand analysis and function design in detail, gives the source codes of relevant interface functions and base algorithm.', 'Manufacturer of thin and ultra-thin non-ferrous metal foils mainly made of copper, copper-alloys, nickel, genuine silver and nickel-silver.', \"All day thy wings have fann'd At that far height, the cold thin atmosphere: Yet stoop not, weary, to the welcome land, Though the dark night is near.\", 'The two other attackers are believed to have tried to enter the terminal, which is protected by heavily armed police and X-ray machines.', 'He became in legitimately through the door of the Law (vv. 1-3).']\n",
      "{'T', '3', 's', 'd', 'l', 'y', 'w', 'S', ':', '1', 'F', 'B', 't', ',', 'Y', 'i', '.', 'O', \"'\", 'k', 'a', 'u', '(', 'C', 'p', 'n', 'f', 'h', 'e', 'A', 'g', 'H', 'M', 'm', 'z', 'b', ')', 'X', '-', 'L', 'r', ' ', 'v', 'c', 'G', 'o'}\n",
      "[' ', '\"', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '“', '…']\n"
     ]
    }
   ],
   "source": [
    "test_texts = []\n",
    "test_characters = set()\n",
    "for text in test_data:\n",
    "    test_texts.append(text['english'])\n",
    "    for char in text['english']:\n",
    "        if char not in test_characters:\n",
    "            test_characters.add(char)\n",
    "\n",
    "print(test_texts)\n",
    "print(test_characters)\n",
    "print(input_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "31d5df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_characters = sorted(list(test_characters))\n",
    "num_test = len(test_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "70c7ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = dict([(char, i) for i, char in enumerate(test_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "89e9a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5fa794b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "28\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "h\n",
      "50\n",
      "a\n",
      "43\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "o\n",
      "57\n",
      "r\n",
      "60\n",
      "m\n",
      "55\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "l\n",
      "54\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "e\n",
      "47\n",
      "e\n",
      "47\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "u\n",
      "63\n",
      "s\n",
      "61\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "e\n",
      "47\n",
      "s\n",
      "61\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "B\n",
      "22\n",
      "r\n",
      "60\n",
      "i\n",
      "51\n",
      "s\n",
      "61\n",
      "t\n",
      "62\n",
      "o\n",
      "57\n",
      "l\n",
      "54\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "u\n",
      "63\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "a\n",
      "43\n",
      "i\n",
      "51\n",
      "l\n",
      "54\n",
      "e\n",
      "47\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "d\n",
      "46\n",
      "e\n",
      "47\n",
      "b\n",
      "44\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "o\n",
      "57\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      " \n",
      "0\n",
      "n\n",
      "56\n",
      "u\n",
      "63\n",
      "m\n",
      "55\n",
      "b\n",
      "44\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "o\n",
      "57\n",
      "f\n",
      "48\n",
      " \n",
      "0\n",
      "p\n",
      "58\n",
      "e\n",
      "47\n",
      "o\n",
      "57\n",
      "p\n",
      "58\n",
      "l\n",
      "54\n",
      "e\n",
      "47\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "c\n",
      "45\n",
      "o\n",
      "57\n",
      "m\n",
      "55\n",
      "p\n",
      "58\n",
      "o\n",
      "57\n",
      "u\n",
      "63\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      "e\n",
      "47\n",
      "d\n",
      "46\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "w\n",
      "65\n",
      "e\n",
      "47\n",
      "n\n",
      "56\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "o\n",
      "57\n",
      " \n",
      "0\n",
      "A\n",
      "21\n",
      "m\n",
      "55\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "i\n",
      "51\n",
      "c\n",
      "45\n",
      "a\n",
      "43\n",
      ".\n",
      "8\n",
      "B\n",
      "22\n",
      "a\n",
      "43\n",
      "s\n",
      "61\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "g\n",
      "49\n",
      " \n",
      "0\n",
      "o\n",
      "57\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "o\n",
      "57\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "s\n",
      "61\n",
      "i\n",
      "51\n",
      "t\n",
      "62\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "e\n",
      "47\n",
      "s\n",
      "61\n",
      "t\n",
      "62\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "o\n",
      "57\n",
      "f\n",
      "48\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "c\n",
      "45\n",
      "h\n",
      "50\n",
      "o\n",
      "57\n",
      "r\n",
      "60\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "u\n",
      "63\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "o\n",
      "57\n",
      "r\n",
      "60\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "o\n",
      "57\n",
      "u\n",
      "63\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "a\n",
      "43\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "c\n",
      "45\n",
      "h\n",
      "50\n",
      "o\n",
      "57\n",
      "r\n",
      "60\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "h\n",
      "50\n",
      "a\n",
      "43\n",
      "v\n",
      "64\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "o\n",
      "57\n",
      "b\n",
      "44\n",
      "v\n",
      "64\n",
      "i\n",
      "51\n",
      "o\n",
      "57\n",
      "u\n",
      "63\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "p\n",
      "58\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "s\n",
      "61\n",
      "t\n",
      "62\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      "s\n",
      "61\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "l\n",
      "54\n",
      "o\n",
      "57\n",
      "s\n",
      "61\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "p\n",
      "58\n",
      "r\n",
      "60\n",
      "o\n",
      "57\n",
      "b\n",
      "44\n",
      "l\n",
      "54\n",
      "e\n",
      "47\n",
      "m\n",
      "55\n",
      " \n",
      "0\n",
      "d\n",
      "46\n",
      "u\n",
      "63\n",
      "r\n",
      "60\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "g\n",
      "49\n",
      " \n",
      "0\n",
      "s\n",
      "61\n",
      "t\n",
      "62\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      "t\n",
      "62\n",
      "c\n",
      "45\n",
      "h\n",
      "50\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "g\n",
      "49\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "l\n",
      "54\n",
      "o\n",
      "57\n",
      "c\n",
      "45\n",
      "k\n",
      "53\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "g\n",
      "49\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      "y\n",
      "67\n",
      "z\n",
      "68\n",
      "e\n",
      "47\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "p\n",
      "58\n",
      "r\n",
      "60\n",
      "o\n",
      "57\n",
      "p\n",
      "58\n",
      "o\n",
      "57\n",
      "s\n",
      "61\n",
      "e\n",
      "47\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "s\n",
      "61\n",
      "e\n",
      "47\n",
      "v\n",
      "64\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      " \n",
      "0\n",
      "s\n",
      "61\n",
      "o\n",
      "57\n",
      "l\n",
      "54\n",
      "u\n",
      "63\n",
      "t\n",
      "62\n",
      "i\n",
      "51\n",
      "o\n",
      "57\n",
      "n\n",
      "56\n",
      "s\n",
      "61\n",
      ".\n",
      "8\n",
      "F\n",
      "26\n",
      "r\n",
      "60\n",
      "o\n",
      "57\n",
      "m\n",
      "55\n",
      " \n",
      "0\n",
      "h\n",
      "50\n",
      "a\n",
      "43\n",
      "i\n",
      "51\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "i\n",
      "51\n",
      "p\n",
      "58\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "i\n",
      "51\n",
      "r\n",
      "60\n",
      "s\n",
      "61\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "e\n",
      "47\n",
      "g\n",
      "49\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "g\n",
      "49\n",
      "r\n",
      "60\n",
      "a\n",
      "43\n",
      "d\n",
      "46\n",
      "u\n",
      "63\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      "l\n",
      "54\n",
      "y\n",
      "67\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "f\n",
      "48\n",
      "t\n",
      "62\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      "l\n",
      "54\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "r\n",
      "60\n",
      "o\n",
      "57\n",
      "u\n",
      "63\n",
      "g\n",
      "49\n",
      "h\n",
      "50\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "r\n",
      "60\n",
      "o\n",
      "57\n",
      "m\n",
      "55\n",
      " \n",
      "0\n",
      "d\n",
      "46\n",
      "o\n",
      "57\n",
      "w\n",
      "65\n",
      "n\n",
      "56\n",
      "w\n",
      "65\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      "d\n",
      "46\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "n\n",
      "56\n",
      "o\n",
      "57\n",
      "d\n",
      "46\n",
      "u\n",
      "63\n",
      "l\n",
      "54\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "c\n",
      "45\n",
      "o\n",
      "57\n",
      "m\n",
      "55\n",
      "b\n",
      "44\n",
      ".\n",
      "8\n",
      "T\n",
      "39\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "s\n",
      "61\n",
      "k\n",
      "53\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "e\n",
      "47\n",
      "g\n",
      "49\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "o\n",
      "57\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "c\n",
      "45\n",
      "l\n",
      "54\n",
      "e\n",
      "47\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "u\n",
      "63\n",
      "p\n",
      "58\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "i\n",
      "51\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "w\n",
      "65\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "w\n",
      "65\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "l\n",
      "54\n",
      "e\n",
      "47\n",
      "f\n",
      "48\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "S\n",
      "38\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "G\n",
      "27\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      "l\n",
      "54\n",
      "e\n",
      "47\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "b\n",
      "44\n",
      "b\n",
      "44\n",
      "e\n",
      "47\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "l\n",
      "54\n",
      "i\n",
      "51\n",
      "b\n",
      "44\n",
      "r\n",
      "60\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      "y\n",
      "67\n",
      ".\n",
      "8\n",
      "O\n",
      "35\n",
      "n\n",
      "56\n",
      "c\n",
      "45\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "m\n",
      "55\n",
      "o\n",
      "57\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "C\n",
      "23\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      "l\n",
      "54\n",
      "l\n",
      "54\n",
      "a\n",
      "43\n",
      "'\n",
      "3\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "a\n",
      "43\n",
      "i\n",
      "51\n",
      "r\n",
      "60\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "g\n",
      "49\n",
      "o\n",
      "57\n",
      "d\n",
      "46\n",
      "m\n",
      "55\n",
      "o\n",
      "57\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      "m\n",
      "55\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      "e\n",
      "47\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "o\n",
      "57\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "h\n",
      "50\n",
      "o\n",
      "57\n",
      "m\n",
      "55\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "m\n",
      "55\n",
      "i\n",
      "51\n",
      "d\n",
      "46\n",
      "n\n",
      "56\n",
      "i\n",
      "51\n",
      "g\n",
      "49\n",
      "h\n",
      "50\n",
      "t\n",
      "62\n",
      ".\n",
      "8\n",
      "T\n",
      "39\n",
      "h\n",
      "50\n",
      "i\n",
      "51\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "p\n",
      "58\n",
      "a\n",
      "43\n",
      "p\n",
      "58\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "t\n",
      "62\n",
      "r\n",
      "60\n",
      "o\n",
      "57\n",
      "d\n",
      "46\n",
      "u\n",
      "63\n",
      "c\n",
      "45\n",
      "e\n",
      "47\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "d\n",
      "46\n",
      "e\n",
      "47\n",
      "m\n",
      "55\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      "y\n",
      "67\n",
      "s\n",
      "61\n",
      "i\n",
      "51\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "u\n",
      "63\n",
      "n\n",
      "56\n",
      "c\n",
      "45\n",
      "t\n",
      "62\n",
      "i\n",
      "51\n",
      "o\n",
      "57\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "d\n",
      "46\n",
      "e\n",
      "47\n",
      "s\n",
      "61\n",
      "i\n",
      "51\n",
      "g\n",
      "49\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "d\n",
      "46\n",
      "e\n",
      "47\n",
      "t\n",
      "62\n",
      "a\n",
      "43\n",
      "i\n",
      "51\n",
      "l\n",
      "54\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "g\n",
      "49\n",
      "i\n",
      "51\n",
      "v\n",
      "64\n",
      "e\n",
      "47\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "s\n",
      "61\n",
      "o\n",
      "57\n",
      "u\n",
      "63\n",
      "r\n",
      "60\n",
      "c\n",
      "45\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "c\n",
      "45\n",
      "o\n",
      "57\n",
      "d\n",
      "46\n",
      "e\n",
      "47\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "o\n",
      "57\n",
      "f\n",
      "48\n",
      " \n",
      "0\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      "l\n",
      "54\n",
      "e\n",
      "47\n",
      "v\n",
      "64\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "t\n",
      "62\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "f\n",
      "48\n",
      "a\n",
      "43\n",
      "c\n",
      "45\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "u\n",
      "63\n",
      "n\n",
      "56\n",
      "c\n",
      "45\n",
      "t\n",
      "62\n",
      "i\n",
      "51\n",
      "o\n",
      "57\n",
      "n\n",
      "56\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "a\n",
      "43\n",
      "s\n",
      "61\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      "g\n",
      "49\n",
      "o\n",
      "57\n",
      "r\n",
      "60\n",
      "i\n",
      "51\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "m\n",
      "55\n",
      ".\n",
      "8\n",
      "M\n",
      "33\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "u\n",
      "63\n",
      "f\n",
      "48\n",
      "a\n",
      "43\n",
      "c\n",
      "45\n",
      "t\n",
      "62\n",
      "u\n",
      "63\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "o\n",
      "57\n",
      "f\n",
      "48\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "u\n",
      "63\n",
      "l\n",
      "54\n",
      "t\n",
      "62\n",
      "r\n",
      "60\n",
      "a\n",
      "43\n",
      "-\n",
      "7\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "n\n",
      "56\n",
      "o\n",
      "57\n",
      "n\n",
      "56\n",
      "-\n",
      "7\n",
      "f\n",
      "48\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "r\n",
      "60\n",
      "o\n",
      "57\n",
      "u\n",
      "63\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "m\n",
      "55\n",
      "e\n",
      "47\n",
      "t\n",
      "62\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "o\n",
      "57\n",
      "i\n",
      "51\n",
      "l\n",
      "54\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "m\n",
      "55\n",
      "a\n",
      "43\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "l\n",
      "54\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "m\n",
      "55\n",
      "a\n",
      "43\n",
      "d\n",
      "46\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "o\n",
      "57\n",
      "f\n",
      "48\n",
      " \n",
      "0\n",
      "c\n",
      "45\n",
      "o\n",
      "57\n",
      "p\n",
      "58\n",
      "p\n",
      "58\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "c\n",
      "45\n",
      "o\n",
      "57\n",
      "p\n",
      "58\n",
      "p\n",
      "58\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "-\n",
      "7\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      "l\n",
      "54\n",
      "o\n",
      "57\n",
      "y\n",
      "67\n",
      "s\n",
      "61\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "n\n",
      "56\n",
      "i\n",
      "51\n",
      "c\n",
      "45\n",
      "k\n",
      "53\n",
      "e\n",
      "47\n",
      "l\n",
      "54\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "g\n",
      "49\n",
      "e\n",
      "47\n",
      "n\n",
      "56\n",
      "u\n",
      "63\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "s\n",
      "61\n",
      "i\n",
      "51\n",
      "l\n",
      "54\n",
      "v\n",
      "64\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "n\n",
      "56\n",
      "i\n",
      "51\n",
      "c\n",
      "45\n",
      "k\n",
      "53\n",
      "e\n",
      "47\n",
      "l\n",
      "54\n",
      "-\n",
      "7\n",
      "s\n",
      "61\n",
      "i\n",
      "51\n",
      "l\n",
      "54\n",
      "v\n",
      "64\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      ".\n",
      "8\n",
      "A\n",
      "21\n",
      "l\n",
      "54\n",
      "l\n",
      "54\n",
      " \n",
      "0\n",
      "d\n",
      "46\n",
      "a\n",
      "43\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "w\n",
      "65\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "g\n",
      "49\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "h\n",
      "50\n",
      "a\n",
      "43\n",
      "v\n",
      "64\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "n\n",
      "56\n",
      "'\n",
      "3\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "A\n",
      "21\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "a\n",
      "43\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "f\n",
      "48\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      "i\n",
      "51\n",
      "g\n",
      "49\n",
      "h\n",
      "50\n",
      "t\n",
      "62\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "c\n",
      "45\n",
      "o\n",
      "57\n",
      "l\n",
      "54\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "t\n",
      "62\n",
      "m\n",
      "55\n",
      "o\n",
      "57\n",
      "s\n",
      "61\n",
      "p\n",
      "58\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      ":\n",
      "19\n",
      " \n",
      "0\n",
      "Y\n",
      "42\n",
      "e\n",
      "47\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "s\n",
      "61\n",
      "t\n",
      "62\n",
      "o\n",
      "57\n",
      "o\n",
      "57\n",
      "p\n",
      "58\n",
      " \n",
      "0\n",
      "n\n",
      "56\n",
      "o\n",
      "57\n",
      "t\n",
      "62\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "w\n",
      "65\n",
      "e\n",
      "47\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      "y\n",
      "67\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "o\n",
      "57\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "w\n",
      "65\n",
      "e\n",
      "47\n",
      "l\n",
      "54\n",
      "c\n",
      "45\n",
      "o\n",
      "57\n",
      "m\n",
      "55\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "l\n",
      "54\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "T\n",
      "39\n",
      "h\n",
      "50\n",
      "o\n",
      "57\n",
      "u\n",
      "63\n",
      "g\n",
      "49\n",
      "h\n",
      "50\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "d\n",
      "46\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      "k\n",
      "53\n",
      " \n",
      "0\n",
      "n\n",
      "56\n",
      "i\n",
      "51\n",
      "g\n",
      "49\n",
      "h\n",
      "50\n",
      "t\n",
      "62\n",
      " \n",
      "0\n",
      "i\n",
      "51\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "n\n",
      "56\n",
      "e\n",
      "47\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      ".\n",
      "8\n",
      "T\n",
      "39\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "w\n",
      "65\n",
      "o\n",
      "57\n",
      " \n",
      "0\n",
      "o\n",
      "57\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "t\n",
      "62\n",
      "t\n",
      "62\n",
      "a\n",
      "43\n",
      "c\n",
      "45\n",
      "k\n",
      "53\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "e\n",
      "47\n",
      "l\n",
      "54\n",
      "i\n",
      "51\n",
      "e\n",
      "47\n",
      "v\n",
      "64\n",
      "e\n",
      "47\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "o\n",
      "57\n",
      " \n",
      "0\n",
      "h\n",
      "50\n",
      "a\n",
      "43\n",
      "v\n",
      "64\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "r\n",
      "60\n",
      "i\n",
      "51\n",
      "e\n",
      "47\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "o\n",
      "57\n",
      " \n",
      "0\n",
      "e\n",
      "47\n",
      "n\n",
      "56\n",
      "t\n",
      "62\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "t\n",
      "62\n",
      "e\n",
      "47\n",
      "r\n",
      "60\n",
      "m\n",
      "55\n",
      "i\n",
      "51\n",
      "n\n",
      "56\n",
      "a\n",
      "43\n",
      "l\n",
      "54\n",
      ",\n",
      "6\n",
      " \n",
      "0\n",
      "w\n",
      "65\n",
      "h\n",
      "50\n",
      "i\n",
      "51\n",
      "c\n",
      "45\n",
      "h\n",
      "50\n",
      " \n",
      "0\n",
      "i\n",
      "51\n",
      "s\n",
      "61\n",
      " \n",
      "0\n",
      "p\n",
      "58\n",
      "r\n",
      "60\n",
      "o\n",
      "57\n",
      "t\n",
      "62\n",
      "e\n",
      "47\n",
      "c\n",
      "45\n",
      "t\n",
      "62\n",
      "e\n",
      "47\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "b\n",
      "44\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "h\n",
      "50\n",
      "e\n",
      "47\n",
      "a\n",
      "43\n",
      "v\n",
      "64\n",
      "i\n",
      "51\n",
      "l\n",
      "54\n",
      "y\n",
      "67\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "r\n",
      "60\n",
      "m\n",
      "55\n",
      "e\n",
      "47\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "p\n",
      "58\n",
      "o\n",
      "57\n",
      "l\n",
      "54\n",
      "i\n",
      "51\n",
      "c\n",
      "45\n",
      "e\n",
      "47\n",
      " \n",
      "0\n",
      "a\n",
      "43\n",
      "n\n",
      "56\n",
      "d\n",
      "46\n",
      " \n",
      "0\n",
      "X\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-7b451418e583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_token_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;31m#test_input_data[i, t, input_token_index[char]] = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'X'"
     ]
    }
   ],
   "source": [
    "# 設定 encoder_input、decoder_input對應的順序    \n",
    "for i, test_text in enumerate(test_texts):\n",
    "    for t, char in enumerate(test_text):\n",
    "        print(char)\n",
    "        print(input_token_index[char])\n",
    "        #test_input_data[i, t, input_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d75ca9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, \"'\": 1, '(': 2, ')': 3, ',': 4, '-': 5, '.': 6, '1': 7, '3': 8, ':': 9, 'A': 10, 'B': 11, 'C': 12, 'F': 13, 'G': 14, 'H': 15, 'L': 16, 'M': 17, 'O': 18, 'S': 19, 'T': 20, 'X': 21, 'Y': 22, 'a': 23, 'b': 24, 'c': 25, 'd': 26, 'e': 27, 'f': 28, 'g': 29, 'h': 30, 'i': 31, 'k': 32, 'l': 33, 'm': 34, 'n': 35, 'o': 36, 'p': 37, 'r': 38, 's': 39, 't': 40, 'u': 41, 'v': 42, 'w': 43, 'y': 44, 'z': 45}\n"
     ]
    }
   ],
   "source": [
    "print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "857a6231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '\"': 1, '&': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, '?': 20, 'A': 21, 'B': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'J': 30, 'K': 31, 'L': 32, 'M': 33, 'N': 34, 'O': 35, 'P': 36, 'R': 37, 'S': 38, 'T': 39, 'U': 40, 'W': 41, 'Y': 42, 'a': 43, 'b': 44, 'c': 45, 'd': 46, 'e': 47, 'f': 48, 'g': 49, 'h': 50, 'i': 51, 'j': 52, 'k': 53, 'l': 54, 'm': 55, 'n': 56, 'o': 57, 'p': 58, 'q': 59, 'r': 60, 's': 61, 't': 62, 'u': 63, 'v': 64, 'w': 65, 'x': 66, 'y': 67, 'z': 68, '—': 69, '“': 70, '…': 71}\n"
     ]
    }
   ],
   "source": [
    "print(input_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9376f81e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1544 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1527 run_step  *\n        outputs = model.predict_step(data)\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1500 predict_step  *\n        return self(x, training=False)\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py:264 assert_input_compatibility  *\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model_7: expected shape=(None, None, 72), found shape=(None, 197, 46)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-ba8903b07115>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# for trying out decoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input sentence:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-2bcecea93a48>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Encode the input as state vectors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mstates_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Generate empty target sequence of length 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1700\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1702\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1703\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m-> 3022\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3440\u001b[1;33m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[0;32m   3441\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3360\u001b[0m           expand_composites=True)\n\u001b[0;32m   3361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3362\u001b[1;33m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[0;32m   3363\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1544 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1527 run_step  *\n        outputs = model.predict_step(data)\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1500 predict_step  *\n        return self(x, training=False)\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\UserTest\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py:264 assert_input_compatibility  *\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model_7: expected shape=(None, None, 72), found shape=(None, 197, 46)\n"
     ]
    }
   ],
   "source": [
    "# 測試100次\n",
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = test_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('*')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    try:\n",
    "        print('Decoded sentence:', decoded_sentence)\n",
    "    except:\n",
    "        # 出現亂碼，以?取代\n",
    "        print('Decoded sentence:', decoded_sentence.encode('ascii', 'replace'))\n",
    "        #print(\"error:\", sys.exc_info()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "22190a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder_input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c937ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
